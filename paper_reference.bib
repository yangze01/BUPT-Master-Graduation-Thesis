%%
%% This is file `example/bare_thesis.bib',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% install/buptgraduatethesis.dtx  (with options: `bare-thesis-bib')
%% 
%% This file is a part of the example of BUPTGraduateThesis.
%% 
@IEEEtranBSTCTL{IEEEexample:BSTcontrol,
  CTLuse_article_number     = "yes",
  CTLuse_paper              = "yes",
  CTLuse_forced_etal        = "no",
  CTLmax_names_forced_etal  = "10",
  CTLnames_show_etal        = "1",
  CTLuse_alt_spacing        = "yes",
  CTLalt_stretch_factor     = "4",
  CTLdash_repeated_names    = "yes",
  CTLname_format_string     = "{f.~}{vv~}{ll}{, jj}",
  CTLname_latex_cmd         = "",
  CTLname_url_prefix        = "[Online]. Available:"
}

@article{kort1957predicting,
  title={Predicting Supreme Court decisions mathematically: A quantitative analysis of the “right to counsel” cases},
  author={Kort, Fred},
  journal={American Political Science Review},
  volume={51},
  number={1},
  pages={1--12},
  year={1957},
  publisher={Cambridge University Press}
}

@article{nagel1963applying,
  title={Applying correlation analysis to case prediction},
  author={Nagel, Stuart S},
  journal={Tex. L. Rev.},
  volume={42},
  pages={1006},
  year={1963},
  publisher={HeinOnline}
}

@article{ulmer1963quantitative,
  title={Quantitative analysis of judicial processes: Some practical and theoretical applications},
  author={Ulmer, S Sidney},
  journal={Law and Contemporary Problems},
  volume={28},
  number={1},
  pages={164--184},
  year={1963},
  publisher={JSTOR}
}


statistic method
We applied the k-Nearest-Neighbor method for determining the prosecution reasons
of the test lawsuits. To this end, we must define a
measure for “similarity” so that we can establish the notion of nearest neighbors of the
current lawsuit 
@inproceedings{LiuC03,
  author    = {Chao{-}Lin Liu and
               Tseng{-}Chung Chang},
  title     = {Some Case-Refinement Strategies for Case-Based Criminal Summary Judgments},
  booktitle = {Foundations of Intelligent Systems, 14th International Symposium,
               {ISMIS} 2003, Maebashi City, Japan, October 28-31, 2003, Proceedings},
  pages     = {285--291},
  year      = {2003},
  doi       = {10.1007/978-3-540-39592-8\_39},
  timestamp = {Mon, 29 May 2017 16:53:44 +0200},
}


@article{keown1980mathematical,
title={Mathematical Models For Legal Prediction},
author={Keown, R},
journal={The John Marshall Journal of Information Technology and Privacy Law},
volume={2},
number={1},
pages={29},
year={1980}}

knn 
@article{LiuCH04,
  author    = {Chao{-}Lin Liu and
               Cheng{-}Tsung Chang and
               Jim{-}How Ho},
  title     = {Case Instance Generation and Refinement for Case-Based Criminal Summary
               Judgments in Chinese},
  journal   = {J. Inf. Sci. Eng.},
  volume    = {20},
  number    = {4},
  pages     = {783--800},
  year      = {2004},
}

tree based
@inproceedings{LiuH06,
  author    = {Chao{-}Lin Liu and
               Chwen{-}Dar Hsieh},
  title     = {Exploring Phrase-Based Classification of Judicial Documents for Criminal
               Charges in Chinese},
  booktitle = {Foundations of Intelligent Systems, 16th International Symposium,
               {ISMIS} 2006, Bari, Italy, September 27-29, 2006, Proceedings},
  pages     = {681--690},
  year      = {2006},
  doi       = {10.1007/11875604\_75},
}

single label
random tree
r, our model is distinctive as it is the first robust, generalized,
and fully predictive model of Supreme Court voting behavior offered to date. Our model predicts six
decades of behavior of thirty Justices appointed by thirteen Presidents. 

Using only information known prior to the Court’s decision, case by case and term by term, we construct
a model that predicts each decision of the Supreme Court of the United States from 1953 - 2013. Leveraging
extremely randomized trees, a particular form of ensemble tree model, we correctly forecast 69.7%
of the Court’s overall affirm / reverse decisions and 70.9% of the votes of individual justices across the
7,700 cases and more than 68,000 justice votes.

@article{KatzBB14,
  author    = {Daniel Martin Katz and
               Michael J. Bommarito II and
               Josh Blackman},
  title     = {Predicting the Behavior of the Supreme Court of the United States:
               {A} General Approach},
  journal   = {CoRR},
  volume    = {abs/1407.6333},
  year      = {2014},
  archivePrefix = {arXiv},
  eprint    = {1407.6333},
}










svm
We developed a mean probability
ensemble system combining the output of multiple SVM classifiers

@article{Sulea2017Exploring,
  title={Exploring the Use of Text Classification in the Legal Domain},
  author={Sulea, Octavia Maria and Zampieri, Marcos and Malmasi, Shervin and Vela, Mihaela and Dinu, Liviu P. and Genabith, Josef Van},
  year={2017},
  archivePrefix = {arXiv},
  arxivId = {1710.09306},
  issn = {16130073},
  volume = {2143},
}


@article{Boella2011Using,
  title={Using classification to support legal knowledge engineers in the Eunomos legal document management system},
  author={Boella, Guido and Caro, Luigi Di and Humphreys, Llio},
  year={2011},
}






alex net
@inproceedings{KrizhevskySH12,
  author    = {Alex Krizhevsky and
               Ilya Sutskever and
               Geoffrey E. Hinton},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 25: 26th Annual
               Conference on Neural Information Processing Systems 2012. Proceedings
               of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States.},
  pages     = {1106--1114},
  year      = {2012},
}

resnet
@article{he2016deep,
title={Deep Residual Learning for Image Recognition},
author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
journal={computer vision and pattern recognition},
pages={770--778},
year={2016}}

object detection
@article{redmon2016you,
title={You Only Look Once: Unified, Real-Time Object Detection},
author={Redmon, Joseph and Divvala, Santosh Kumar and Girshick, Ross B and Farhadi, Ali},
journal={computer vision and pattern recognition},
pages={779--788},
year={2016}}
    

audio
@article{hinton2012deep,
title={Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups},
author={Hinton, Geoffrey E and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdelrahman and Jaitly, Navdeep and Senior, Andrew W and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and others},
journal={IEEE Signal Processing Magazine},
volume={29},
number={6},
pages={82--97},
year={2012}}

@article{amodei2016deep,
title={Deep speech 2: end-to-end speech recognition in English and mandarin},
author={Amodei, Dario and Ananthanarayanan, Sundaram and Anubhai, Rishita and Bai, Jingliang and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Cheng, Qiang and Chen, Guoliang and others},
journal={international conference on machine learning},
pages={173--182},
year={2016}}

NLP
@article{kim2016character-aware,
title={Character-aware neural language models},
author={Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M},
journal={national conference on artificial intelligence},
pages={2741--2749},
year={2016}}

@article{kumar2016ask,
title={Ask me anything: dynamic memory networks for natural language processing},
author={Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
journal={international conference on machine learning},
pages={1378--1387},
year={2016}}


not cite we have implemented two unsupervised baseline models(tf-idf and LDA-based information retrieval),
and a supervised model, Ranking SVM, for the task. The features of the model are a set of words.
@article{kim2014legal, 
title={Legal Question Answering Using Ranking SVM and Syntactic/Semantic Similarity},
author={Kim, Miyoung and Xu, Ying and Goebel, Randy},
pages={244--258},
year={2014}}

not cite We propose a model including three stages: legal information retrieval,
legal textual entailment and legal text answering. In the first stage, a mixed
size n-gram model built from morphological analysis is used to rank and select
relevant articles corresponding to a legal question; next, pairs of questions and
retrieved articles are judged by a machine learning algorithm trained on lexical
features and Distributional Semantic similarity, to decide whether the questions
can be answered positively or negatively by the retrieved articles;
@article{carvalho2015lexical,
title={Lexical-Morphological Modeling for Legal Text Analysis},
author={Carvalho, Danilo S and Nguyen, Minhtien and Tran, Chienxuan and Nguyen, Minh Le},
journal={international symposium on artificial intelligence},
pages={295--311},
year={2015}}

not cite binary svm We formulate a binary classification task where the input of our classifiers is 
the textual content extracted from a case and the target output is the actual 
judgment as to whether there has been a violation of an article of the convention 
of human rights. Textual information is represented using contiguous word sequences, 
i.e., N-grams, and topics. 
@article{AletrasTPL16,
  author    = {Nikolaos Aletras and
               Dimitrios Tsarapatsanis and
               Daniel Preotiuc{-}Pietro and
               Vasileios Lampos},
  title     = {Predicting judicial decisions of the European Court of Human Rights:
               a Natural Language Processing perspective},
  journal   = {PeerJ Computer Science},
  volume    = {2},
  pages     = {e93},
  year      = {2016},
  doi       = {10.7717/peerj-cs.93},
}

not cite Batch and Online, respectively. In the Batch process, three outputs
are generated to be used in the Online process. The first output is a classification model that classifies cases to statutes,
and is produced by adopting a SVM (support vector machine) classifier. In the second output, all statutes are represented as
statute vectors. The last output is a set of association rules, which show what statutes frequently occur together, and is generated
from the training collection of judgments. In the Online process, the classification model is adopted to acquire the
prediction of top k1 statutes for the user query. Then, the NGD (Normalized Google Distance) method (Cilibrasi & Vitanyi,
2007; Evangelista & Kjos-Hanssen, 2006) is used to perform terms transformation between the statutes and user query,
and the top k2 most similar statutes are selected.
@article{liu2015predicting,
title={Predicting associated statutes for legal problems},
author={Liu, Yihung and Chen, Yenliang and Ho, Wuliang},
journal={Information Processing and Management},
volume={51},
number={1},
pages={194--211},
year={2015}}
    


DNN propose an attention-based neural network method to jointly model the charge prediction 
task and the relevant article extraction task in a unified framework.
@article{luo2017learning,
title={Learning to Predict Charges for Criminal Cases with Legal Basis.},
author={Luo, Bingfeng and Feng, Yansong and Xu, Jianbo and Zhang, Xiang and Zhao, Dongyan},
journal={empirical methods in natural language processing},
pages={2727--2736},
year={2017}}

DNN We formalize the dependencies among subtasks as a Directed Acyclic Graph (DAG) and incorporates 
topological multi-task learning framework into judgment prediction.
@inproceedings{ZhongGTX0S18,
  author    = {Haoxi Zhong and
               Zhipeng Guo and
               Cunchao Tu and
               Chaojun Xiao and
               Zhiyuan Liu and
               Maosong Sun},
  title     = {Legal Judgment Prediction via Topological Learning},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural
               Language Processing, Brussels, Belgium, October 31 - November 4, 2018},
  pages     = {3540--3549},
  year      = {2018},
}

DNN we propose an attribute-attentive charge prediction model to infer the attributes 
and charges simultaneously.
@InProceedings{C18-1041,
  author = 	"Hu, Zikun
		and Li, Xiang
		and Tu, Cunchao
		and Liu, Zhiyuan
		and Sun, Maosong",
  title = 	"Few-Shot Charge Prediction with Discriminative Legal Attributes",
  booktitle = 	"Proceedings of the 27th International Conference on Computational Linguistics",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"487--498",
  location = 	"Santa Fe, New Mexico, USA",
  url = 	"http://aclweb.org/anthology/C18-1041"
}

DNN my paper
@inproceedings{WangYNZZN18,
  author    = {Pengfei Wang and
               Ze Yang and
               Shuzi Niu and
               Yongfeng Zhang and
               Lei Zhang and
               ShaoZhang Niu},
  title     = {Modeling Dynamic Pairwise Attention for Crime Classification over
               Legal Articles},
  booktitle = {The 41st International {ACM} {SIGIR} Conference on Research {\&}
               Development in Information Retrieval, {SIGIR} 2018, Ann Arbor, MI,
               USA, July 08-12, 2018},
  pages     = {485--494},
  year      = {2018},
  doi       = {10.1145/3209978.3210057},
}

BP-MLL
@Article{zhang2006multilabel,
  author  = {Zhang, Min Ling and Zhou, Zhi Hua},
  title   = {Multilabel Neural Networks with Applications to Functional Genomics and Text Categorization},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  year    = {2006},
  volume  = {18},
  number  = {10},
  pages   = {1338-1351},
}

BiLSTM
@article{graves2005framewise,
  title={Framewise phoneme classification with bidirectional LSTM and other neural network architectures},
  author={Graves, Alex and Schmidhuber, J{\"u}rgen},
  journal={Neural Networks},
  volume={18},
  number={5-6},
  pages={602--610},
  year={2005},
  publisher={Elsevier}
}


@inproceedings{Elisseeff2001A,
  title={A kernel method for multi-labelled classification},
  author={Elisseeff, Andr and Weston, Jason},
  booktitle={International Conference on Neural Information Processing Systems: Natural and Synthetic},
  pages={681-687},
  year={2001},
}

@article{Brinker2008Multilabel,
  title={Multilabel classification via calibrated label ranking},
  author={Brinker, Klaus},
  journal={Machine Learning},
  volume={73},
  number={2},
  pages={133-153},
  year={2008},
}

mlknn
@article{zhang2007ml,
  title={ML-KNN: A lazy learning approach to multi-label learning},
  author={Zhang, Min-Ling and Zhou, Zhi-Hua},
  journal={Pattern recognition},
  volume={40},
  number={7},
  pages={2038--2048},
  year={2007},
  publisher={Elsevier}
}

@article{ReadPHF11,
  author    = {Jesse Read and
               Bernhard Pfahringer and
               Geoff Holmes and
               Eibe Frank},
  title     = {Classifier chains for multi-label classification},
  journal   = {Machine Learning},
  volume    = {85},
  number    = {3},
  pages     = {333--359},
  year      = {2011},
  doi       = {10.1007/s10994-011-5256-5},
}

@article{ZhangZ07,
  author    = {Min{-}Ling Zhang and
               Zhi{-}Hua Zhou},
  title     = {{ML-KNN:} {A} lazy learning approach to multi-label learning},
  journal   = {Pattern Recognition},
  volume    = {40},
  number    = {7},
  pages     = {2038--2048},
  year      = {2007},
  doi       = {10.1016/j.patcog.2006.12.019},
}

attention model
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@incollection{NIPS2014_5346,
title = {Sequence to Sequence Learning with Neural Networks},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {3104--3112},
year = {2014},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf}
}



@inproceedings{LuongPM15,
  author    = {Thang Luong and
               Hieu Pham and
               Christopher D. Manning},
  title     = {Effective Approaches to Attention-based Neural Machine Translation},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2015, Lisbon, Portugal, September 17-21,
               2015},
  pages     = {1412--1421},
  year      = {2015},
}

TextCNN
@inproceedings{Kim14,
  author    = {Yoon Kim},
  title     = {Convolutional Neural Networks for Sentence Classification},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2014, October 25-29, 2014, Doha, Qatar,
               {A} meeting of SIGDAT, a Special Interest Group of the {ACL}},
  pages     = {1746--1751},
  year      = {2014},
}

Word
@inproceedings{BordesGWB12,
  author    = {Antoine Bordes and
               Xavier Glorot and
               Jason Weston and
               Yoshua Bengio},
  title     = {Joint Learning of Words and Meaning Representations for Open-Text
               Semantic Parsing},
  booktitle = {Proceedings of the Fifteenth International Conference on Artificial
               Intelligence and Statistics, {AISTATS} 2012, La Palma, Canary Islands,
               Spain, April 21-23, 2012},
  pages     = {127--135},
  year      = {2012},
}

visual attention
@inproceedings{MnihHGK14,
  author    = {Volodymyr Mnih and
               Nicolas Heess and
               Alex Graves and
               Koray Kavukcuoglu},
  title     = {Recurrent Models of Visual Attention},
  booktitle = {Advances in Neural Information Processing Systems 27: Annual Conference
               on Neural Information Processing Systems 2014, December 8-13 2014,
               Montreal, Quebec, Canada},
  pages     = {2204--2212},
  year      = {2014}
}


extension attention

@inproceedings{LuongSLVZ15,
  author    = {Thang Luong and
               Ilya Sutskever and
               Quoc V. Le and
               Oriol Vinyals and
               Wojciech Zaremba},
  title     = {Addressing the Rare Word Problem in Neural Machine Translation},
  booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational
               Linguistics and the 7th International Joint Conference on Natural
               Language Processing of the Asian Federation of Natural Language Processing,
               {ACL} 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers},
  pages     = {11--19},
  year      = {2015},
}


attention in CNN
@article{YinSXZ16,
  author    = {Wenpeng Yin and
               Hinrich Sch{\"{u}}tze and
               Bing Xiang and
               Bowen Zhou},
  title     = {{ABCNN:} Attention-Based Convolutional Neural Network for Modeling
               Sentence Pairs},
  journal   = {{TACL}},
  volume    = {4},
  pages     = {259--272},
  year      = {2016},
}

self attention
@article{LinFSYXZB17,
  author    = {Zhouhan Lin and
               Minwei Feng and
               C{\'{\i}}cero Nogueira dos Santos and
               Mo Yu and
               Bing Xiang and
               Bowen Zhou and
               Yoshua Bengio},
  title     = {A Structured Self-attentive Sentence Embedding},
  journal   = {CoRR},
  volume    = {abs/1703.03130},
  year      = {2017},
  archivePrefix = {arXiv},
  eprint    = {1703.03130},
}

attention is all you need
@inproceedings{VaswaniSPUJGKP17,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention is All you Need},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
               on Neural Information Processing Systems 2017, 4-9 December 2017,
               Long Beach, CA, {USA}},
  pages     = {6000--6010},
  year      = {2017},
}

BERT
@article{Jacob181004805,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
}

role tagging self-attention
@inproceedings{TanWXCS18,
  author    = {Zhixing Tan and
               Mingxuan Wang and
               Jun Xie and
               Yidong Chen and
               Xiaodong Shi},
  title     = {Deep Semantic Role Labeling With Self-Attention},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence,
               (AAAI-18), the 30th innovative Applications of Artificial Intelligence
               (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in
               Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
               2-7, 2018},
  pages     = {4929--4936},
  year      = {2018},
 }

 medical
@inproceedings{VergaSM18,
  author    = {Patrick Verga and
               Emma Strubell and
               Andrew McCallum},
  title     = {Simultaneously Self-Attending to All Mentions for Full-Abstract Biological
               Relation Extraction},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume
               1 (Long Papers)},
  pages     = {872--884},
  year      = {2018}
  }


datasets
@inproceedings{zhong2018legal,
  title={Legal Judgment Prediction via Topological Learning},
  author={Zhong, Haoxi and Zhipeng, Guo and Tu, Cunchao and Xiao, Chaojun and Liu, Zhiyuan and Sun, Maosong},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={3540--3549},
  year={2018}
}

FastText
@InProceedings{joulin2017bag,
  title={Bag of Tricks for Efficient Text Classification},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  month={April},
  year={2017},
  publisher={Association for Computational Linguistics},
  pages={427--431},
}

attention over attention
@inproceedings{CuiCWWLH17,
  author    = {Yiming Cui and
               Zhipeng Chen and
               Si Wei and
               Shijin Wang and
               Ting Liu and
               Guoping Hu},
  title     = {Attention-over-Attention Neural Networks for Reading Comprehension},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2017, Vancouver, Canada, July 30 - August 4, Volume
               1: Long Papers},
  pages     = {593--602},
  year      = {2017},
}
